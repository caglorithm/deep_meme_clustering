{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:17:34.205912Z",
     "start_time": "2018-06-12T22:17:34.198435Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "import imagecluster as ic\n",
    "import phashlib as ph\n",
    "\n",
    "from imagecluster import common as co\n",
    "from imagecluster import imagecluster as ic\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:17:34.398267Z",
     "start_time": "2018-06-12T22:17:34.387065Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(ic)\n",
    "importlib.reload(ph)\n",
    "importlib.reload(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:17:34.563645Z",
     "start_time": "2018-06-12T22:17:34.554977Z"
    }
   },
   "outputs": [],
   "source": [
    "ic_base_dir = 'imagecluster'\n",
    "modelname = 'ResNet50'\n",
    "input_size = 224\n",
    "importlib.reload(ic)\n",
    "\n",
    "imagedir = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:17:34.995196Z",
     "start_time": "2018-06-12T22:17:34.903855Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_dataset(imagedir, modelname = 'ResNet50', input_size = 224):\n",
    "    \"\"\"\n",
    "    processes a list of files (filenames) \n",
    "    \n",
    "    1 - calculates sha256 hash and renames files to hash\n",
    "    2 - crops out image from meme and copies into ./cropped/\n",
    "    3 - calculates phash using the imagehash library\n",
    "    4 - calculates dnn fingerprint using keras and tensorflow\n",
    "    6 - does the same for cropped versions\n",
    "    7 - applies a clustering algorithm on fingerprints of cropped images\n",
    "    8 - plots all members of all clusters into a jpg file and saves results\n",
    "    \n",
    "    - returns a pandas dataframe with the information\n",
    "    \"\"\"\n",
    "    files = co.get_files(imagedir)\n",
    "    print(\"> Renaming {} files (to sha256 hash)\".format(len(files)))\n",
    "    files, hashes = co.rename_files(files, imagedir)\n",
    "    print(\"done.\")\n",
    "    \n",
    "    # create pandas dataframe containing all data\n",
    "    df = pd.DataFrame(index=hashes)\n",
    "    df['filename'] = files\n",
    "    df['hash'] = hashes\n",
    "    \n",
    "    print(\"> Phashing {} files\".format(len(files)))\n",
    "    phashes = ph.return_phashes(files)\n",
    "    df['phash'] = phashes\n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(\"> Cropping and copying all images\")\n",
    "    df = co.crop_images(df)\n",
    "    print(\"done.\")        \n",
    "    \n",
    "    print(\"> Loading Keras model {}\".format(modelname))\n",
    "    model, getFingerprint = ph.get_model(modelname=modelname)\n",
    "    # construct fingerprint model (second to last layer)\n",
    "    #getFingerprint = K.function([model.layers[0].input],\n",
    "    #                              [model.layers[-2].output])\n",
    "    \n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(\"> Running images through DNN {}\".format(modelname))\n",
    "    # get fingerprints\n",
    "    fps, preds, labels = ph.fingerprints(files, model, getFingerprint, size=(input_size,input_size), modelname=modelname)\n",
    "    df['fingerprints'] = fps\n",
    "    df['labels'] = labels\n",
    "    \n",
    "    print(\"> Running CROPPED images through DNN {}\".format(modelname))\n",
    "    # get fingerprints\n",
    "    cfps, cpreds, clabels = ph.fingerprints(files, model, getFingerprint, size=(input_size,input_size), modelname=modelname)\n",
    "    df['cropped_fingerprints'] = cfps\n",
    "    df['cropped_labels'] = clabels\n",
    "    \n",
    "    print(\"done.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:24:32.413685Z",
     "start_time": "2018-06-12T22:17:35.068979Z"
    }
   },
   "outputs": [],
   "source": [
    "dbfn = os.path.join(imagedir, ic_base_dir, 'db.pk')\n",
    "if not os.path.exists(dbfn):\n",
    "    os.makedirs(os.path.dirname(dbfn), exist_ok=True)\n",
    "    print(\"no fingerprints database found in {}\".format(dbfn))\n",
    "    #fps = ic.fingerprints(files, model, size=(input_size,input_size), modelname=modelname)\n",
    "    df_exists = 'df' in locals() or 'df' in globals()\n",
    "    if not df_exists:\n",
    "        print(\"Running processing pipeline ...\")\n",
    "        df = process_dataset(imagedir)\n",
    "    else:\n",
    "        print(\"df exists already.\")\n",
    "    print(\"writing {}\".format(dbfn))\n",
    "    co.write_pk(df, dbfn)\n",
    "else:\n",
    "    print(\"loading fingerprints database {} ...\".format(dbfn))\n",
    "    df = co.read_pk(dbfn)\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:24:43.404366Z",
     "start_time": "2018-06-12T22:24:43.279074Z"
    }
   },
   "outputs": [],
   "source": [
    "fingerprint_column = 'cropped_fingerprints'\n",
    "\n",
    "fingerprintdict = df.set_index('filename')[fingerprint_column].to_dict()\n",
    "#clusters = ic.cluster(fingerprints, 0.6)\n",
    "ic.make_links(ic.cluster(fingerprintdict, 0.6), os.path.join(imagedir, ic_base_dir, 'clusters'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:36:22.362578Z",
     "start_time": "2018-06-12T21:36:22.353932Z"
    }
   },
   "source": [
    "### Save results in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:24:47.153764Z",
     "start_time": "2018-06-12T22:24:47.031350Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "fps = df[fingerprint_column]\n",
    "\n",
    "dfps = distance.pdist(np.array(list(fps)), metric='euclidean')\n",
    "\n",
    "sim = 0.6 \n",
    "\n",
    "Z = hierarchy.linkage(dfps, method='average', metric='euclidean')\n",
    "cut = hierarchy.fcluster(Z, t=dfps.max()*(1.0-sim), criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:24:47.392835Z",
     "start_time": "2018-06-12T22:24:47.383771Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cluster'] = cut\n",
    "#co.write_pk(df, dbfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:36:23.666744Z",
     "start_time": "2018-06-12T21:36:22.456113Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fingerprintdict = df.set_index('filename')['fingerprints'].to_dict()\n",
    "\n",
    "dfps = list(fingerprintdict.values())\n",
    "files = list(fingerprintdict.keys())\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(dfps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:36:23.766397Z",
     "start_time": "2018-06-12T21:36:23.670237Z"
    }
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=40, min_samples=2, metric='euclidean', n_jobs=4).fit(dfps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:36:23.776194Z",
     "start_time": "2018-06-12T21:36:23.769460Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"{} clusters, {} outliers\".format(n_clusters_, list(labels).count(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: find best eps value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epss = range(1, 50)\n",
    "nclusts = []\n",
    "for eps in tqdm(epss, total=len(epss)):\n",
    "    \n",
    "    db = DBSCAN(eps=eps, min_samples=2, metric='euclidean', n_jobs=4).fit(dfps)\n",
    "    labels = db.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    nclusts.append(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epss, nclusts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:24:51.422320Z",
     "start_time": "2018-06-12T22:24:51.317299Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "% matplotlib inline\n",
    "\n",
    "def plotfiles(files, plot = True, filename = '', labels=[]):\n",
    "    nrows = max(2, int(np.ceil(np.sqrt(len(files)))))\n",
    "    ncols = max(2, int(np.floor(np.sqrt(len(files)))))\n",
    "    nimgs = nrows * nrows\n",
    "    if len(files) < 3: nrows = 1\n",
    "    f, axs = plt.subplots(nrows, ncols, figsize=(ncols*3, nrows*3), dpi=300)\n",
    "    for n in range(nimgs):\n",
    "        row, col = (n)//(ncols), (n)%(ncols)\n",
    "        if n < len(files):\n",
    "            try:\n",
    "                img=mpimg.imread(files[n])\n",
    "                bbox_props = dict(boxstyle=\"circle\", fc=\"w\", ec=\"0.5\", pad=0.2, alpha=0.9)\n",
    "                if nrows == 1:\n",
    "                    axs[n].imshow(img)\n",
    "                    if len(labels)<=len(files): axs[n].text(0.05, 0.05, labels[n], transform=axs[n].transAxes, \n",
    "                                                            bbox={'facecolor':'white', 'alpha':0.8, 'pad':2}, fontsize=6)\n",
    "                else:\n",
    "                    axs[row, col].imshow(img)\n",
    "                    if len(labels)<=len(files): axs[row, col].text(0.05, 0.05, labels[n], transform=axs[row, col].transAxes, \n",
    "                                                                   bbox={'facecolor':'white', 'alpha':0.8, 'pad':2}, fontsize=6)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            if nrows == 1:\n",
    "                axs[n].axis('off')\n",
    "            else:\n",
    "                axs[row, col].axis('off')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    if len(filename) > 0:\n",
    "        plt.savefig(filename)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:25:29.913406Z",
     "start_time": "2018-06-12T22:24:56.646526Z"
    }
   },
   "outputs": [],
   "source": [
    "# save results on disk as jpgs\n",
    "clusterdir = os.path.join(imagedir, ic_base_dir, 'clusters')\n",
    "clusterlist = list(df['cluster'])\n",
    "unique_clusters = np.unique(df['cluster'])\n",
    "cut = df['cluster']\n",
    "\n",
    "for nclust in unique_clusters:\n",
    "    clustersize = clusterlist.count(nclust)\n",
    "    if clustersize > 1 and clustersize < 500:\n",
    "        print(nclust, clustersize)\n",
    "        clusterdf = df[df['cluster'] == nclust]\n",
    "        \n",
    "        labels = list(clusterdf['labels'])\n",
    "        #labels = [result[0] for result in [label[0] for label in clusterdf['labels']]]\n",
    "        print(labels)\n",
    "        \n",
    "        clusterfile = os.path.join(clusterdir, str(clustersize) + '_' + str(nclust) + '.jpg')\n",
    "        plotfiles(list(clusterdf['filename']), plot=False, filename=clusterfile)\n",
    "        print(clusterfile)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:37:21.221471Z",
     "start_time": "2018-06-12T21:37:20.132994Z"
    }
   },
   "outputs": [],
   "source": [
    "#files = ['/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/00a38bfafff15902662a0d03c6427bca6770f1ba4936674f2865bf8d87143123.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/1cd7722f5d4ca8de4e9321ae542e1351d062cc5cdc0ca02952e7ca59551406b2.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/26aaf025766038a76e799b40b923ec379228e5aa861080221c160ba702128cd1.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/44579f6bfff124e4de2ba4eb89368bd4d65de31252ffa2683cbf009b5cbe6b40.png', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/7c21827cb33454d280fb99999eff069658ce14ac00a12a7faeb12f32ad988790.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/8e6d237951f8af40836787f4098dfd436bd01f6f54f70a984f9ec12e7167060a.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/bdca8d59b55bfa5d0387d906469ad667e1c3b2e89c7ecdebd8ee197f7dbcf532.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/bdd7263028bf7e7d1daed68f8a06ecafc4e5be5f70b146874886dfb1fd10e5e7.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/d0bdbac22427d292acb6d0f0aaef67f894f345ec98171e778f5a9c212e9cfdbf.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/f1e160315801a95a8d5d6fdf089f7b933ede021efbd7bdb90d7ab81fd31d4c04.jpg']\n",
    "files = list(df['filename'])[:2]\n",
    "plotfiles(files, filename='1.jpg', labels=['hi', 'u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:37:32.905041Z",
     "start_time": "2018-06-12T21:37:26.302547Z"
    }
   },
   "outputs": [],
   "source": [
    "#files = ['/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/00a38bfafff15902662a0d03c6427bca6770f1ba4936674f2865bf8d87143123.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/1cd7722f5d4ca8de4e9321ae542e1351d062cc5cdc0ca02952e7ca59551406b2.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/26aaf025766038a76e799b40b923ec379228e5aa861080221c160ba702128cd1.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/44579f6bfff124e4de2ba4eb89368bd4d65de31252ffa2683cbf009b5cbe6b40.png', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/7c21827cb33454d280fb99999eff069658ce14ac00a12a7faeb12f32ad988790.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/8e6d237951f8af40836787f4098dfd436bd01f6f54f70a984f9ec12e7167060a.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/bdca8d59b55bfa5d0387d906469ad667e1c3b2e89c7ecdebd8ee197f7dbcf532.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/bdd7263028bf7e7d1daed68f8a06ecafc4e5be5f70b146874886dfb1fd10e5e7.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/d0bdbac22427d292acb6d0f0aaef67f894f345ec98171e778f5a9c212e9cfdbf.jpg', '/Users/caglar/Downloads/Web/rips/small/imagecluster/clusters/cluster_with_10/cluster_0/f1e160315801a95a8d5d6fdf089f7b933ede021efbd7bdb90d7ab81fd31d4c04.jpg']\n",
    "files = list(df['filename'])[:100:10]\n",
    "plotfiles(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: image preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:37:49.278306Z",
     "start_time": "2018-06-12T21:37:49.266265Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image_for_cropping(img):\n",
    "    img = img - np.mean(img)\n",
    "    img /= np.std(img)\n",
    "    #img = img[:,:,0]/3 - img[:,:,1]/3 - img[:,:,2]/3\n",
    "    img = np.std(img, axis=2)\n",
    "    # convert image to some grayscale mush\n",
    "    #for i in range(1,2):\n",
    "    #    img = img[:, :, 0]/3 - img[:, :, i]/3\n",
    "    #plt.imshow(img, cmap='gray')\n",
    "    #plt.colorbar()\n",
    "    #plt.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:37:59.167154Z",
     "start_time": "2018-06-12T21:37:59.087584Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_crop_bbox(img):\n",
    "    \n",
    "    img = preprocess_image_for_cropping(img)\n",
    "    \n",
    "    yrange, xrange = img.shape[:2]\n",
    "    \n",
    "    croplines_x = []\n",
    "    croplines_y = []\n",
    "    \n",
    "    mean_x = [[],[]]\n",
    "    for x in range(xrange):\n",
    "        # extract cross sections to analyze\n",
    "        filterline = np.abs(img[:, x])\n",
    "        # interpolate line\n",
    "        boxwidth = 10\n",
    "        box = np.ones(boxwidth)/boxwidth\n",
    "        filterline = np.convolve(filterline, box, mode='same')\n",
    "        \n",
    "        filter_threshold = np.mean(img)/10\n",
    "        \n",
    "        # find pixels where threshold is crossed \n",
    "        threshold_crossings = np.where(np.array(filterline)>filter_threshold)[0]\n",
    "        \n",
    "        # take mean of the found borders across image\n",
    "        if len(threshold_crossings) > 0:\n",
    "            croplines_x.append([threshold_crossings[0], threshold_crossings[-1]])\n",
    "    if len(croplines_x)>1:\n",
    "        mean_x = np.median(np.array(croplines_x), axis=0).astype(int)\n",
    "        \n",
    "    mean_y = [[],[]]\n",
    "    for y in range(yrange):\n",
    "        # extract cross sections to analyze\n",
    "        filterline = np.abs(img[y, :])\n",
    "        # interpolate line\n",
    "        boxwidth = 10\n",
    "        box = np.ones(boxwidth)/boxwidth\n",
    "        filterline = np.convolve(filterline, box, mode='same')\n",
    "\n",
    "        filter_threshold = np.mean(img)/10\n",
    "\n",
    "        threshold_crossings = np.where(np.array(filterline)>filter_threshold)[0]\n",
    "        if len(threshold_crossings) > 0:\n",
    "            croplines_y.append([threshold_crossings[0], threshold_crossings[-1]])\n",
    "    if len(croplines_y)>1:\n",
    "        mean_y = np.median(np.array(croplines_y), axis=0).astype(int)\n",
    "    return mean_y, mean_x # threshold crossings on y axis are x values to crop and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:38:21.411829Z",
     "start_time": "2018-06-12T21:38:21.399725Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_croplines(croplines_x, croplines_y, img):\n",
    "    if (len(croplines_x)==2 and len(croplines_y)==2):\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.hlines(croplines_y[0], croplines_x[0], croplines_x[1], color='g', lw=5)\n",
    "        plt.hlines(croplines_y[1], croplines_x[0], croplines_x[1], color='y', lw=5)\n",
    "\n",
    "        plt.vlines(croplines_x[0], croplines_y[0], croplines_y[1], color='r', lw=5)\n",
    "        plt.vlines(croplines_x[1], croplines_y[0], croplines_y[1], color='b', lw=5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop images and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:16:27.095308Z",
     "start_time": "2018-06-12T22:16:27.027566Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_images(df):\n",
    "    cropped_folder = os.path.join(imagedir, 'cropped/')\n",
    "    if not os.path.exists(cropped_folder):\n",
    "        os.makedirs(os.path.dirname(cropped_folder), exist_ok=True)\n",
    "    if 'cropped_filename' not in df:\n",
    "        df['cropped_filename'] = None\n",
    "    for file in tqdm(df.index, total=len(df.index)):\n",
    "        pil_img=Image.open(df.loc[file]['filename'])\n",
    "        fhash = df.loc[file]['hash']\n",
    "        cropped_fname = os.path.join(imagedir, 'cropped/', fhash + '.jpg')\n",
    "\n",
    "        pil_img.thumbnail((input_size, input_size), Image.ANTIALIAS)\n",
    "        img = np.array(pil_img)\n",
    "        origimg = img.copy()\n",
    "        croplines_x, croplines_y = get_crop_bbox(img)\n",
    "\n",
    "        w, h = pil_img.size\n",
    "        if len(croplines_x) is not 2: \n",
    "            croplines_x = [0, w]\n",
    "            print(\"couldn't crop {} in x-axis\".format(file))\n",
    "        if len(croplines_y) is not 2: \n",
    "            croplines_y = [0, h]\n",
    "            print(\"couldn't crop {} in y-axis\".format(file))\n",
    "\n",
    "        #plot_croplines(croplines_x, croplines_y, img)    \n",
    "        pil_img = pil_img.crop((croplines_x[0], croplines_y[0], croplines_x[1], croplines_y[1]))\n",
    "        pil_img = pil_img.convert('RGB') \n",
    "\n",
    "        pil_img.save(cropped_fname)\n",
    "\n",
    "        df.loc[file]['cropped_filename'] = cropped_fname\n",
    "\n",
    "        #plt.imshow(pil_img)\n",
    "        #plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
